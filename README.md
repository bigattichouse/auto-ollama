<div align="center">
<h1>Auto-Ollama</h1>
<p>Seamlessly Inference Large Language Models (LLMs) Locally with a Single Command</p>
</div>

## Overview

Auto-Ollama is a toolkit designed to simplify the inference of Large Language Models (LLMs) directly on your local environment. With an emphasis on ease of use and flexibility, Auto-Ollama supports a wide range of models, providing tools for both direct usage and conversion of models into an efficient format for local deployment.

## Getting Started

### Prerequisites

Before you begin, ensure you have the following installed:
- Git (for cloning the repository)
- Bash environment (Linux/MacOS terminal or Windows Subsystem for Linux)
- Python 3.9 or newer

### Installation

Clone the repository to get started with Auto-Ollama:

```bash
git clone https://github.com/monk1337/auto-ollama.git
cd auto-ollama
